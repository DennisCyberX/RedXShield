from nbformat import v4 as nbf
import json

# Create a Jupyter notebook with training steps for RedXShield
notebook = nbf.new_notebook()
notebook.cells = [
    nbf.new_markdown_cell("# RedXShield â€“ WHOIS NLP & Hybrid Classifier Training Notebook\n"
                          "This notebook trains the BERT-based NLP model on WHOIS data."),
    nbf.new_code_cell("!pip install transformers datasets scikit-learn torch"),
    nbf.new_code_cell("import pandas as pd\n"
                      "import torch\n"
                      "from sklearn.model_selection import train_test_split\n"
                      "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n"
                      "from datasets import Dataset"),
    nbf.new_markdown_cell("## Step 1: Load WHOIS Dataset"),
    nbf.new_code_cell("df = pd.read_csv(\"whois_data.csv\")\n"
                      "df = df[['text', 'label']].dropna()\n"
                      "print(f\"Total samples: {len(df)}\")"),
    nbf.new_markdown_cell("## Step 2: Train/Test Split"),
    nbf.new_code_cell("train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
                      "train_dataset = Dataset.from_pandas(train_df)\n"
                      "test_dataset = Dataset.from_pandas(test_df)"),
    nbf.new_markdown_cell("## Step 3: Tokenization"),
    nbf.new_code_cell("tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
                      "def tokenize_function(example):\n"
                      "    return tokenizer(example['text'], padding=\"max_length\", truncation=True)\n"
                      "train_dataset = train_dataset.map(tokenize_function, batched=True)\n"
                      "test_dataset = test_dataset.map(tokenize_function, batched=True)"),
    nbf.new_markdown_cell("## Step 4: Load BERT Model"),
    nbf.new_code_cell("model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"),
    nbf.new_markdown_cell("## Step 5: Training Configuration"),
    nbf.new_code_cell("training_args = TrainingArguments(\n"
                      "    output_dir=\"./models/whois_bert\",\n"
                      "    evaluation_strategy=\"epoch\",\n"
                      "    num_train_epochs=3,\n"
                      "    per_device_train_batch_size=8,\n"
                      "    per_device_eval_batch_size=8,\n"
                      "    save_steps=500,\n"
                      "    save_total_limit=1,\n"
                      "    logging_dir='./logs',\n"
                      "    logging_steps=100,\n"
                      "    load_best_model_at_end=True\n"
                      ")"),
    nbf.new_markdown_cell("## Step 6: Train the Model"),
    nbf.new_code_cell("trainer = Trainer(\n"
                      "    model=model,\n"
                      "    args=training_args,\n"
                      "    train_dataset=train_dataset,\n"
                      "    eval_dataset=test_dataset\n"
                      ")\n"
                      "trainer.train()"),
    nbf.new_markdown_cell("## Step 7: Save the Model"),
    nbf.new_code_cell("model.save_pretrained(\"./models/whois_bert\")\n"
                      "tokenizer.save_pretrained(\"./models/whois_bert\")"),
    nbf.new_markdown_cell("## Step 8: Evaluate the Model"),
    nbf.new_code_cell("eval_result = trainer.evaluate()\n"
                      "print(\"Evaluation Result:\", eval_result)")
]

# Save the notebook to a file
notebook_path = "/mnt/data/training_notebook.ipynb"
with open(notebook_path, "w") as f:
    json.dump(notebook, f)

notebook_path
