from nbformat import write, v4

# Define notebook cells
cells = [
    v4.new_markdown_cell("# RedXShield â€“ WHOIS NLP & Hybrid Classifier Training Notebook\n"
                         "This notebook trains the BERT-based NLP model on WHOIS data."),
    v4.new_code_cell("!pip install transformers datasets scikit-learn torch"),
    v4.new_code_cell(
        "import pandas as pd\n"
        "import torch\n"
        "from sklearn.model_selection import train_test_split\n"
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n"
        "from datasets import Dataset"
    ),
    v4.new_code_cell(
        "# Load WHOIS dataset\n"
        "df = pd.read_csv(\"whois_data.csv\")\n"
        "df = df[['text', 'label']].dropna()\n"
        "print(f\"Total samples: {len(df)}\")"
    ),
    v4.new_code_cell(
        "# Split into train/test sets\n"
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
        "train_dataset = Dataset.from_pandas(train_df)\n"
        "test_dataset = Dataset.from_pandas(test_df)"
    ),
    v4.new_code_cell(
        "# Tokenization\n"
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
        "def tokenize_function(example):\n"
        "    return tokenizer(example['text'], padding=\"max_length\", truncation=True)\n"
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n"
        "test_dataset = test_dataset.map(tokenize_function, batched=True)"
    ),
    v4.new_code_cell(
        "# Load BERT model\n"
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
    ),
    v4.new_code_cell(
        "# Set training arguments\n"
        "training_args = TrainingArguments(\n"
        "    output_dir=\"./models/whois_bert\",\n"
        "    evaluation_strategy=\"epoch\",\n"
        "    num_train_epochs=3,\n"
        "    per_device_train_batch_size=8,\n"
        "    per_device_eval_batch_size=8,\n"
        "    save_steps=500,\n"
        "    save_total_limit=1,\n"
        "    logging_dir='./logs',\n"
        "    logging_steps=100,\n"
        "    load_best_model_at_end=True\n"
        ")"
    ),
    v4.new_code_cell(
        "# Train the model\n"
        "trainer = Trainer(\n"
        "    model=model,\n"
        "    args=training_args,\n"
        "    train_dataset=train_dataset,\n"
        "    eval_dataset=test_dataset\n"
        ")\n"
        "trainer.train()"
    ),
    v4.new_code_cell(
        "# Save the model\n"
        "model.save_pretrained(\"./models/whois_bert\")\n"
        "tokenizer.save_pretrained(\"./models/whois_bert\")"
    ),
    v4.new_code_cell(
        "# Evaluate the model\n"
        "eval_result = trainer.evaluate()\n"
        "print(\"Evaluation Result:\", eval_result)"
    )
]
